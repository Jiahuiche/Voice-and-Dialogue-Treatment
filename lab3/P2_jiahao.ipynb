{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout, Conv1D, TimeDistributed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jiahao_funcs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f238f064",
   "metadata": {},
   "source": [
    "# Data and preproces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81786bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown \"https://drive.google.com/uc?id=1u2wzXvsuscLeFHwXcDwMDaNDy0u_99-t\"\n",
    "!tar -zxf nlu_ATIS_data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c559bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv', header=None)\n",
    "val_data = train_data.tail(900)\n",
    "train_data = pd.read_csv('./data/train.csv', header=None, nrows=4078)\n",
    "test_data = pd.read_csv('./data/test.csv', header=None)\n",
    "print('-------------- Dataset original --------------')\n",
    "print('Training size:', len(train_data))\n",
    "print('Validation dataset size:', len(val_data))\n",
    "print('Test dataset size:', len(test_data))\n",
    "data = preprocess_entity_recognition(train_data, val_data, test_data, num_words=500)\n",
    "print('-------------- Dataset preprocesado entity recognition --------------')\n",
    "print('Vocab size:', data['vocab_size'])\n",
    "print('Maxlen:', data['maxlen'])\n",
    "print('Num classes:', data['num_classes'])\n",
    "print(data[\"train_X\"].shape, data[\"train_y\"].shape)\n",
    "print(data[\"val_X\"].shape, data[\"val_y\"].shape)\n",
    "print(data[\"test_X\"].shape, data[\"test_y\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238f08b",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c725d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(num_classes, vocab_size, maxlen, embedding_dim, num_head = 4, ff_dim = 256):\n",
    "    model = Sequential()\n",
    "    model.add(TokenAndPositionEmbedding(maxlen, vocab_size, embedding_dim))\n",
    "    model.add(TransformerBlock(embedding_dim, num_head, ff_dim))\n",
    "    model.add(keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "    return model\n",
    "results = provar_embeddings(model_build, preprocessed_data=data, batch_size=32, epochs=30, \n",
    "                            embedding_dims=[32, 64, 128, 256, 384, 512], patience=5, runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a59a4",
   "metadata": {},
   "source": [
    "# Balanceo de clases\n",
    "Usar el class_weight en model.fit(class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculate_class_weights(data)\n",
    "print('Class weights:')\n",
    "for class_index, weight in class_weights.items():\n",
    "    print(f'{class_index}: {np.round(weight, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcfc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_weights_for_sequences(data, class_weights):\n",
    "    # y_data has shape (num_samples, maxlen, num_classes) and is one-hot encoded\n",
    "    # class_weights is a dictionary mapping class index to weight\n",
    "\n",
    "    num_samples, maxlen, num_classes = data['train_y'].shape[0], data['maxlen'], data['num_classes']\n",
    "    # Initialize sample_weights array with shape (num_samples, maxlen)\n",
    "    sample_weights_output = np.zeros((num_samples, maxlen))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for j in range(maxlen):\n",
    "            # Get the true class index for the current token (one-hot to int)\n",
    "            class_index = np.argmax(data['train_y'][i, j, :])\n",
    "            # Look up the weight for this class, default to 1.0 if not found\n",
    "            sample_weights_output[i, j] = class_weights.get(class_index, 1.0)\n",
    "\n",
    "    return sample_weights_output\n",
    "\n",
    "# Compute sample weights for training and validation data\n",
    "train_sample_weights = compute_sample_weights_for_sequences(data, class_weights)\n",
    "\n",
    "print('Shape of train_sample_weights:', train_sample_weights.shape)\n",
    "print('First 5 sample weights for train_data:')\n",
    "print(train_sample_weights[:5]) # Display first 5x5 segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb94a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build(num_classes, vocab_size, maxlen, embedding_dim, num_head = 4, ff_dim = 256):\n",
    "    model = Sequential()\n",
    "    model.add(TokenAndPositionEmbedding(maxlen, vocab_size, embedding_dim))\n",
    "    model.add(TransformerBlock(embedding_dim, num_head, ff_dim))\n",
    "    model.add(keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "    return model\n",
    "results = probar_sample_weights(model_build, preprocessed_data=data, batch_size=16, epochs=15, \n",
    "                               sample_weights_list=[None, train_sample_weights], patience=5, runs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
